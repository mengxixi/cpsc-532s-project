%------------------------------------------------------------------------
\section{Related Work}
\subsection{Visual Grounding}
\label{sec: vg_attention}

Many approaches have explored using attention with different levels of granularity in the visual grounding task. In particular, \cite{rohrbach2016grounding} applies attention weights on the aggregated features of proposal bounding boxes and input phrases, and attempts to reconstruct the phrase using these attention weights. The loss is applied on the reconstructed phrase, and hence the attention weights will be maximized for the correct box. \cite{xiao2017weakly} instead performs grounding at the pixel level with spatial attention masks without any explicit bounding box annotations. The spatial attention masks are generated by exploiting the hierarchical structure of the parse tree from an image caption. In order to enforce joint learning on the multimodal input source, \cite{deng2018visual} applies attention on the query, image and object regions simultaneously to obtain a more compact correspondence between the correct pairs. In a recent work, \cite{dogan2019neural} formulates grounding of phrases in a sequential and contextual manner, leveraging comprehensive global information, and enabling well performing multi-instance grounding.

\subsection{Multimodal Learning with Graph Embeddings}
The omnipresence of graph structured data has promoted considerable research effort devoted to representation learning of the graph nodes. The goal is to project node features onto a latent space while preserving the pairwise relationship encoded in the underlying graph. These embeddings can then be used in various downstream machine learning tasks such as node classification, link prediction and neighborhood identification \cite{hamilton2017representation}. Recent works often use a deep neural network architecture which allows node features to be incorporated into the embeddings. One of the most popular approaches is to use a neighborhood aggregation function (similar to message-passing) to learn a local representation, possibly inspired by convolutional filters in computer vision. In particular, the graph convolutional network (GCN) was introduced to perform transductive learning with node features and the graph's adjacency matrix \cite{kipf2016semi}.


Several works have taken advantage of an underlying graph structure of a problem in a multimodal learning setting. In the task of visual question answering, \cite{teney2017graph} encodes both the question and the input scene image as separate graphs that capture the spatial arrangement and contextual information to enhance their prediction over the output vocabulary. However, this method requires knowing the image regions of interest and their pairwise relationships beforehand, which are often unavailable in a real-world setting. To overcome this problem, scene graph generation networks such as \cite{yang2018graph} has been proposed, which motivates our work to improve visual grounding systems using scene graph embeddings. 

\subsection{Scene Graph Generation}

Visual scene graph generation is a task that requires not only accurately detecting objects in an image, but also correctly predicting the subtle interactions between them. The vertices in a scene graph are the individual scene object bounding boxes, while the edges represent their relationships as predicates. For instance, in an image depicting ``a man wearing a shirt sitting on a bench", the corresponding scene graph should have vertices \texttt{\{Man, Shirt, Bench\}} and edges \texttt{\{(Man, Wearing, Shirt), (Man, Sitting, Bench)\}}. Recently, \cite{xu2017scene} proposed a message passing architecture utilizing messages with contextual information between a pair of bipartite graphs, of which the set of edges are refined iteratively. \cite{zhang2017relationship} developed a Relationship Proposal Network (Rel-PN) that directly predicts relationships by scoring the likelihood of the triplet \texttt{(subject, object, and relation)}. \cite{yang2018graph} designed a Relation Proposal Network (RePN) that proposes potential edges among object bounding boxes and uses an attentional GCN (aGCN) model to aggregate node and edge features to generate scene graph.  \cite{gu2019scene} tried to tackle issues from noisy and biased annotations in the Visual Genome dataset \cite{krishna2017visual} by regularizing the model via reconstructing the original image from scene graph predictions. \cite{NIPS2018_7951} formulated the scene graph generation task as a score-based structure prediction problem and designed a network so that it is invariant to permutation in the input structure. 
