\section{Introduction}
Visual grounding is the task of associating textual input with the corresponding regions in a given image. The problem has attracted much attention in recent years as it plays an important role in applications such as image retrieval and intelligent dialogue systems. Many of the recent works such as \cite{rohrbach2016grounding, deng2018visual} perform visual grounding by utilizing unstructured visual features from pretrained object classification networks such as VGG16 \cite{simonyan2014very}. However, in reality, the representation of an object in a scene should be structurally constrained by its surroundings. As an example, the feature representation of ``a man sitting alone in a bus" should be inherently distinct from the same man ``sitting on the beach with his family". 

This observation motivates our work on augmenting visual grounding systems with scene graph embeddings. Unfortunately, most datasets in the computer vision community do not come with scene graphs that annotate relationships between objects in an image. Some prior works such as \cite{teney2017graph} in visual question answering attempted to encode relationship between objects by relying solely on spatial relationships (such as relative positions of bounding boxes). However, spatial information might not be sufficiently rich in expressing contextual pairwise relationships between objects. On the other hand, there have been on-going research in scene graph generation which attempt to assign more meaningful predicate relationships between objects in an image \cite{xu2017scene, zhang2017relationship, yang2018graph, NIPS2018_7951, gu2019scene} . Therefore, in this work, we propose a method that enhance visual grounding by incorporating scene object embeddings through an auxiliary scene graph generation task. 

Our contributions are summarized as follows. First, we show that the use of scene graph embedding can significantly improve the performance of a simple visual grounding system, and our proposed model achieves an accuracy close to that of the state-of-the-art. We perform ablation studies to demonstrate the impact of adopting our framework, and finally we explore the possibility of augmentation with sentence graphs as well as multi-instance grounding.